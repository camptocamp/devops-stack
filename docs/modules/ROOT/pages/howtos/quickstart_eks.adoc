= Deploying the DevOps Stack to EKS
:ci-variables: aws
:terraform-provider: AWS
:terraform-provider-link: https://registry.terraform.io/providers/hashicorp/aws/latest/docs

== Prerequisites

- Access to API keys allowing to create required resources in AWS,
- Access to GitLab or GitHub (only supported CI/CD for now),
- Knowledge of https://terraform.io[Terraform] basics


== Create your Terraform root module

Camptocamp's DevOps stack is a Terraform composition module
has to be instantiated from the Terraform root module.

Here is a minimal working example:

```hcl
# terraform/main.tf

locals {
  cluster_name = "my-cluster"
}

data "aws_availability_zones" "available" {}

module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "2.66.0"

  name = local.cluster_name
  cidr = "10.0.0.0/16"

  azs = data.aws_availability_zones.available.names

  private_subnets = [
    "10.0.1.0/24",
    "10.0.2.0/24",
    "10.0.3.0/24",
  ]

  public_subnets = [
    "10.0.11.0/24",
    "10.0.12.0/24",
    "10.0.13.0/24",
  ]

  # NAT Gateway Scenarios : One NAT Gateway per availability zone
  enable_nat_gateway     = true
  single_nat_gateway     = false
  one_nat_gateway_per_az = true

  enable_dns_hostnames = true
  enable_dns_support   = true

  private_subnet_tags = {
    "kubernetes.io/cluster/default"   = "shared"
    "kubernetes.io/role/internal-elb" = "1"
  }

  public_subnet_tags = {
    "kubernetes.io/role/elb" = "1"
  }
}

module "cluster" {
  source = "git::https://github.com/camptocamp/camptocamp-devops-stack.git//modules/eks/aws?ref=v0.26.0"

  cluster_name = local.cluster_name
  vpc_id       = module.vpc.vpc_id

  worker_groups = [
    {
      instance_type        = "m5a.large"
      asg_desired_capacity = 2
      asg_max_size         = 3
    }
  ]

  base_domain     = "example.com"

  cognito_user_pool_id     = aws_cognito_user_pool.pool.id
  cognito_user_pool_domain = aws_cognito_user_pool_domain.pool_domain.domain
}

resource "aws_cognito_user_pool" "pool" {
  name = "pool"
}

resource "aws_cognito_user_pool_domain" "pool_domain" {
  domain       = "pool-domain"
  user_pool_id = aws_cognito_user_pool.pool.id
}
```

include::partial$pipeline_outputs.adoc[]

include::partial$terraform_backend.adoc[]

include::partial$deploy_workstation.adoc[]



== Deploying from pipelines

When using pipelines, the DevOps Stack runs a dry-run on Merge Request and applies
the modification on commit on a protected branch.

include::partial$pipelines_gitlab.adoc[ci-variables, terraform-provider, terraform-provider-link]


include::partial$pipelines_github.adoc[ci-variables, terraform-provider, terraform-provider-link]


== Recovering the kubeconfig for EKS

. Make sure your AWS credentials file (`~/.aws/credentials` on Linux) contains the access key that has been used to create the cluster:
+
```ini
[<ACCOUNT_NAME>]
aws_access_key_id = <YOUR_ACCESS_KEY_ID>
aws_secret_access_key = <YOUR_SECRET_ACCESS_KEY>
region = <YOUR_REGION>
```

. Update your kubeconfig using the following command:
+
```bash
aws --profile <ACCOUNT_NAME> eks --region <YOUR_REGION> update-kubeconfig --name <CLUSTER_NAME>
```

Then, you should be able to use `kubectl`.



include::partial$inspect_apps.adoc[]

include::partial$tf_destroy.adoc[]
